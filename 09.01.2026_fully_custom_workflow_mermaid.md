# Fully Custom Question Generation - Workflow Flowchart

## Complete Workflow Diagram

```mermaid
flowchart TD
    Start([Start: Fully Custom Question Generation]) --> Config[Load Configuration]
    
    Config --> ConfigDetails{Configuration Settings}
    ConfigDetails --> CD1[questions_per_document: 20]
    ConfigDetails --> CD2[num_spatial_regions: 4]
    ConfigDetails --> CD3[chunk_size: 1500]
    ConfigDetails --> CD4[multi_turn_ratio: 40%]
    
    CD1 --> InitLLM[Initialize LLMs]
    CD2 --> InitLLM
    CD3 --> InitLLM
    CD4 --> InitLLM
    
    InitLLM --> InitDetails[Base LLM + Structured LLMs]
    InitDetails --> LLM1[question_llm with GeneratedQuestion]
    InitDetails --> LLM2[followup_llm with FollowupQuestion]
    
    LLM1 --> LoadDocs[Load Documents from ./documents/]
    LLM2 --> LoadDocs
    
    LoadDocs --> FileTypes{Process File Types}
    
    FileTypes --> PDF[PDF Files: PyPDFLoader]
    FileTypes --> DOCX[DOCX Files: Docx2txtLoader]
    FileTypes --> TEXT[MD/TXT: read_text UTF-8]
    
    PDF --> DocsList[Combined Documents List]
    DOCX --> DocsList
    TEXT --> DocsList
    
    DocsList --> CheckDocs{Documents Found?}
    CheckDocs -->|No| EndError([End: No Documents])
    CheckDocs -->|Yes| GroupFiles[Group by Source File]
    
    GroupFiles --> ChunkDocs[Chunk Documents]
    
    ChunkDocs --> ChunkProcess[RecursiveCharacterTextSplitter<br/>chunk_size=1500<br/>overlap=300]
    
    ChunkProcess --> ChunkMeta[Add Chunk Metadata<br/>chunk_index, total_chunks]
    
    ChunkMeta --> SpatialSample[Spatial Sampling]
    
    SpatialSample --> SpatialProcess{For Each Document}
    
    SpatialProcess --> DivideRegions[Divide into 4 Spatial Regions]
    DivideRegions --> SampleRegions[Sample Evenly from Each Region]
    
    SampleRegions --> CheckChunks{Enough Chunks?}
    
    CheckChunks -->|Yes: n_chunks >= target| UseChunks[Use Sampled Chunks]
    CheckChunks -->|No: n_chunks < target| CycleChunks[Cycle Through Chunks<br/>Repeat: 0,1,2,0,1,2...]
    
    UseChunks --> GenPrep[Prepare Generation]
    CycleChunks --> GenPrep
    
    GenPrep --> TypeSeq[Create Question Type Sequence<br/>Based on Distribution]
    
    TypeSeq --> TypeDist{Question Types}
    TypeDist --> T1[chatbot_style: 10%]
    TypeDist --> T2[direct_factual: 10%]
    TypeDist --> T3[procedural: 15%]
    TypeDist --> T4[scenario: 20%]
    TypeDist --> T5[analytical: 10%]
    TypeDist --> T6[compliance: 5%]
    TypeDist --> T7[descriptive: 10%]
    TypeDist --> T8[multi_hop: 10%]
    TypeDist --> T9[comparative: 5%]
    TypeDist --> T10[conditional: 5%]
    
    T1 --> MultiTurn[Determine Multi-turn Questions<br/>40% get follow-ups]
    T2 --> MultiTurn
    T3 --> MultiTurn
    T4 --> MultiTurn
    T5 --> MultiTurn
    T6 --> MultiTurn
    T7 --> MultiTurn
    T8 --> MultiTurn
    T9 --> MultiTurn
    T10 --> MultiTurn
    
    MultiTurn --> GenLoop{For Each Chunk + Type}
    
    GenLoop --> GenQuestion[Generate Question with Pydantic]
    
    GenQuestion --> StructPrompt[Structured Prompt<br/>Context + Type + Requirements]
    
    StructPrompt --> LLMInvoke[question_llm.invoke]
    
    LLMInvoke --> PydanticReturn[Returns: GeneratedQuestion Object<br/>- question<br/>- answer<br/>- question_type<br/>- complexity<br/>- references]
    
    PydanticReturn --> AddMeta[Add Metadata<br/>source_file, region_id]
    
    AddMeta --> IsMultiTurn{Is Multi-turn?}
    
    IsMultiTurn -->|No| StoreSingle[Store as Single-turn]
    IsMultiTurn -->|Yes| GenFollowup[Generate Follow-up Questions]
    
    GenFollowup --> FollowupPrompt[Follow-up Prompt<br/>Context + Initial Q&A]
    
    FollowupPrompt --> FollowupLLM[followup_llm.invoke]
    
    FollowupLLM --> FollowupReturn[Returns: FollowupQuestion Object<br/>- followup_question<br/>- followup_answer<br/>- followup_type]
    
    FollowupReturn --> StoreMulti[Store as Multi-turn<br/>With Follow-ups]
    
    StoreSingle --> NextChunk{More Chunks?}
    StoreMulti --> NextChunk
    
    NextChunk -->|Yes| GenLoop
    NextChunk -->|No| EnsureCount[Ensure Exact Count<br/>Trim if > target]
    
    EnsureCount --> NextDoc{More Documents?}
    
    NextDoc -->|Yes| SpatialProcess
    NextDoc -->|No| FlattenData[Flatten to DataFrame]
    
    FlattenData --> FlatProcess[Process Each Question]
    
    FlatProcess --> InitialQ[Add Initial Question<br/>turn_number=1]
    InitialQ --> HasFollowup{Has Follow-ups?}
    
    HasFollowup -->|No| NextQFlat{More Questions?}
    HasFollowup -->|Yes| AddFollowups[Add Follow-up Rows<br/>turn_number=2,3...<br/>parent_question_id]
    
    AddFollowups --> NextQFlat
    NextQFlat -->|Yes| FlatProcess
    NextQFlat -->|No| CreateDF[Create Final DataFrame]
    
    CreateDF --> DFCols[Columns:<br/>question_id, question, answer<br/>source_file, region_id<br/>question_type, complexity<br/>conversation_type, turn_number<br/>is_followup, parent_question_id]
    
    DFCols --> Analysis[Coverage Analysis]
    
    Analysis --> Stats[Calculate Statistics<br/>- Total questions<br/>- Type distribution<br/>- Complexity distribution<br/>- Multi-turn count]
    
    Stats --> Export[Export Results]
    
    Export --> E1[testset_custom_full.csv<br/>Complete dataset]
    Export --> E2[testset_custom_simple.csv<br/>For RAG evaluation]
    Export --> E3[testset_conversation_chains.csv<br/>Multi-turn only]
    Export --> E4[testset_summary.json<br/>Statistics]
    
    E1 --> Visualize[Create Visualizations]
    E2 --> Visualize
    E3 --> Visualize
    E4 --> Visualize
    
    Visualize --> V1[Question Type Distribution]
    Visualize --> V2[Conversation Type Pie Chart]
    Visualize --> V3[Complexity Distribution]
    Visualize --> V4[Questions by Source File]
    
    V1 --> SaveViz[testset_analysis.png]
    V2 --> SaveViz
    V3 --> SaveViz
    V4 --> SaveViz
    
    SaveViz --> End([End: Complete!<br/>All files in ./outputs/])
    
    style Start fill:#90EE90
    style End fill:#90EE90
    style EndError fill:#FFB6C1
    style GenQuestion fill:#87CEEB
    style GenFollowup fill:#87CEEB
    style LLMInvoke fill:#FFD700
    style FollowupLLM fill:#FFD700
    style PydanticReturn fill:#98FB98
    style FollowupReturn fill:#98FB98
    style Export fill:#FFA07A
    style Visualize fill:#DDA0DD
```

---

## Key Decision Points Flowchart

```mermaid
flowchart TD
    Start([Question Generation Process]) --> LoadCheck{Documents Loaded?}
    
    LoadCheck -->|No| Error1([Error: No Documents])
    LoadCheck -->|Yes| ChunkCheck{Chunks Available?}
    
    ChunkCheck --> Compare{n_chunks vs target}
    
    Compare -->|n_chunks >= target| Strategy1[Strategy: Sample & Use Once<br/>Pick best chunks<br/>1 question per chunk]
    Compare -->|n_chunks < target| Strategy2[Strategy: Cycle Chunks<br/>Repeat chunks<br/>Multiple questions per chunk]
    
    Strategy1 --> Generate[Generate Questions]
    Strategy2 --> Generate
    
    Generate --> CountCheck{Question Count?}
    
    CountCheck -->|< target| Warning[âš ï¸ Under target<br/>Use all available]
    CountCheck -->|= target| Perfect[âœ… Exact count]
    CountCheck -->|> target| Trim[Trim to exact target]
    
    Warning --> Output[Final Output]
    Perfect --> Output
    Trim --> Output
    
    Output --> End([End])
    
    style Start fill:#90EE90
    style End fill:#90EE90
    style Error1 fill:#FFB6C1
    style Strategy1 fill:#87CEEB
    style Strategy2 fill:#FFD700
    style Perfect fill:#90EE90
    style Warning fill:#FFFF99
    style Trim fill:#FFA07A
```

---

## Pydantic Structured Output Flow

```mermaid
flowchart LR
    Prompt[Question Prompt<br/>Context + Type] --> LLM[question_llm.invoke]
    
    LLM --> Validate{Schema Valid?}
    
    Validate -->|No| Retry[LangChain Auto-retry]
    Retry --> LLM
    
    Validate -->|Yes| Pydantic[GeneratedQuestion Object]
    
    Pydantic --> Fields[Validated Fields:<br/>âœ“ question: str<br/>âœ“ answer: str<br/>âœ“ question_type: str<br/>âœ“ complexity: easy/medium/hard<br/>âœ“ references: List]
    
    Fields --> Use[Use in Application<br/>Type-safe access]
    
    style Prompt fill:#87CEEB
    style LLM fill:#FFD700
    style Validate fill:#FFA07A
    style Retry fill:#FFB6C1
    style Pydantic fill:#90EE90
    style Fields fill:#98FB98
    style Use fill:#DDA0DD
```

---

## Chunk Cycling Logic (Small Documents)

```mermaid
flowchart TD
    Start([Small Document<br/>2 pages, 3 chunks]) --> Target[Target: 20 questions]
    
    Target --> Check{3 chunks < 20 questions?}
    
    Check -->|Yes| Cycle[Cycle Strategy]
    
    Cycle --> Pattern[Cycling Pattern:<br/>0,1,2,0,1,2,0,1,2,0,1,2...]
    
    Pattern --> Positions[20 Positions Created]
    
    Positions --> Breakdown[Chunk Usage:<br/>Chunk 0: Used 7 times<br/>Chunk 1: Used 7 times<br/>Chunk 2: Used 6 times]
    
    Breakdown --> DiffTypes[Different Question Types<br/>Each Position]
    
    DiffTypes --> Example[Example:<br/>Position 0: chatbot_style<br/>Position 3: multi_hop<br/>Position 6: scenario<br/>Same chunk, different questions!]
    
    Example --> Result[âœ… Result: 20 questions<br/>from 3 unique chunks]
    
    style Start fill:#87CEEB
    style Cycle fill:#FFD700
    style Pattern fill:#98FB98
    style Result fill:#90EE90
```

---

## Multi-turn Conversation Flow

```mermaid
flowchart TD
    Initial[Generate Initial Question] --> Multi{Multi-turn Selected?<br/>40% probability}
    
    Multi -->|No| Single[Mark as Single-turn<br/>conversation_type='single_turn']
    
    Multi -->|Yes| NumTurns[Determine Number of Turns<br/>Random 1 to max_turns]
    
    NumTurns --> GenFollowup1[Generate Follow-up 1]
    
    GenFollowup1 --> Followup1[followup_llm.invoke<br/>Context + Initial Q&A]
    
    Followup1 --> Return1[FollowupQuestion Object<br/>followup_question<br/>followup_answer<br/>followup_type]
    
    Return1 --> MoreTurns{More Turns?}
    
    MoreTurns -->|Yes| GenFollowup2[Generate Follow-up 2]
    MoreTurns -->|No| Store[Store Multi-turn<br/>conversation_type='multi_turn']
    
    GenFollowup2 --> Followup2[Generate Additional Follow-ups]
    Followup2 --> Store
    
    Single --> Next([Continue to Next Question])
    Store --> Next
    
    style Initial fill:#87CEEB
    style GenFollowup1 fill:#FFD700
    style GenFollowup2 fill:#FFD700
    style Return1 fill:#98FB98
    style Store fill:#90EE90
```

---

## File Type Processing

```mermaid
flowchart LR
    Files[./documents/] --> PDF[*.pdf]
    Files --> DOCX[*.docx]
    Files --> MD[*.md]
    Files --> TXT[*.txt]
    
    PDF --> PDFLoader[PyPDFLoader<br/>Page-by-page]
    DOCX --> DOCXLoader[Docx2txtLoader<br/>Text extraction]
    MD --> TextRead[read_text<br/>UTF-8 encoding]
    TXT --> TextRead
    
    PDFLoader --> Meta1[Add Metadata:<br/>source_file<br/>file_type='pdf']
    DOCXLoader --> Meta2[Add Metadata:<br/>source_file<br/>file_type='docx']
    TextRead --> Meta3[Add Metadata:<br/>source_file<br/>file_type='md' or 'txt']
    
    Meta1 --> Combine[Combined Documents List]
    Meta2 --> Combine
    Meta3 --> Combine
    
    Combine --> Process[Continue Processing]
    
    style Files fill:#87CEEB
    style PDFLoader fill:#FFD700
    style DOCXLoader fill:#FFD700
    style TextRead fill:#98FB98
    style Combine fill:#90EE90
```

---

## Spatial Coverage Strategy

```mermaid
flowchart TD
    Doc[Document: 100 chunks] --> Divide[Divide into 4 Regions]
    
    Divide --> R1[Region 1: Chunks 0-24<br/>BEGINNING]
    Divide --> R2[Region 2: Chunks 25-49<br/>EARLY-MIDDLE]
    Divide --> R3[Region 3: Chunks 50-74<br/>LATE-MIDDLE]
    Divide --> R4[Region 4: Chunks 75-99<br/>END]
    
    R1 --> S1[Sample 5 chunks<br/>Evenly spaced]
    R2 --> S2[Sample 5 chunks<br/>Evenly spaced]
    R3 --> S3[Sample 5 chunks<br/>Evenly spaced]
    R4 --> S4[Sample 5 chunks<br/>Evenly spaced]
    
    S1 --> Total[Total: 20 sampled chunks]
    S2 --> Total
    S3 --> Total
    S4 --> Total
    
    Total --> Coverage[âœ… Complete Document Coverage<br/>All sections represented]
    
    style Doc fill:#87CEEB
    style R1 fill:#FFB6C1
    style R2 fill:#98FB98
    style R3 fill:#FFD700
    style R4 fill:#DDA0DD
    style Coverage fill:#90EE90
```

---

## Output Files Structure

```mermaid
flowchart TD
    Generate[Question Generation Complete] --> Outputs[./outputs/]
    
    Outputs --> CSV1[testset_custom_full.csv]
    Outputs --> CSV2[testset_custom_simple.csv]
    Outputs --> CSV3[testset_conversation_chains.csv]
    Outputs --> JSON[testset_summary.json]
    Outputs --> PNG[testset_analysis.png]
    
    CSV1 --> Full[All Columns:<br/>question_id, question, answer<br/>source_file, region_id<br/>question_type, complexity<br/>conversation_type<br/>turn_number, is_followup<br/>parent_question_id, references]
    
    CSV2 --> Simple[RAG Evaluation Format:<br/>question<br/>ground_truth<br/>source]
    
    CSV3 --> Conv[Multi-turn Only:<br/>Filtered for conversations<br/>with follow-ups]
    
    JSON --> Stats[Statistics:<br/>total_questions<br/>type distribution<br/>complexity distribution<br/>conversation counts]
    
    PNG --> Viz[4 Visualizations:<br/>1. Question Types<br/>2. Single vs Multi-turn<br/>3. Complexity<br/>4. Questions by Source]
    
    style Outputs fill:#90EE90
    style CSV1 fill:#87CEEB
    style CSV2 fill:#FFD700
    style CSV3 fill:#98FB98
    style JSON fill:#FFA07A
    style PNG fill:#DDA0DD
```

---

## How to Use These Diagrams

### **Option 1: Copy to Mermaid Live Editor**
1. Go to https://mermaid.live/
2. Copy any diagram code above
3. Paste and view interactive diagram
4. Export as PNG/SVG

### **Option 2: Use in Markdown**
Just paste the code blocks in any Markdown file that supports Mermaid (GitHub, GitLab, Notion, etc.)

### **Option 3: VS Code**
Install "Markdown Preview Mermaid Support" extension and view in preview

---

## Diagram Legend

| Color | Meaning |
|-------|---------|
| ðŸŸ¢ Green | Start/End/Success |
| ðŸ”µ Blue | Processing Steps |
| ðŸŸ¡ Yellow | Decision/Strategy |
| ðŸŸ  Orange | Export/Output |
| ðŸŸ£ Purple | Visualization |
| ðŸ”´ Red | Error/Warning |
| ðŸŒ¸ Pink | Regions/Sections |

---

*All diagrams are in Mermaid format and ready to use!*
