{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f5442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877b74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e524818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# RAGAS (v0.1.21)\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc0f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================\n",
    "# PYDANTIC MODELS FOR STRUCTURED OUTPUT\n",
    "# =====================================================================\n",
    "\n",
    "class QuestionClassification(BaseModel):\n",
    "    \"\"\"Structured classification for questions\"\"\"\n",
    "    question_style: Literal[\n",
    "        \"chatbot_style\", \"direct_factual\", \"procedural\", \"scenario\",\n",
    "        \"analytical\", \"compliance\", \"descriptive\", \"multi_hop\",\n",
    "        \"comparative\", \"conditional\"\n",
    "    ] = Field(description=\"The style/category of the question\")\n",
    "    \n",
    "    question_type: Literal[\n",
    "        \"chatbot_style\", \"direct_factual\", \"procedural\", \"scenario\",\n",
    "        \"analytical\", \"compliance\", \"descriptive\", \"multi_hop\",\n",
    "        \"comparative\", \"conditional\"\n",
    "    ] = Field(description=\"The type of the question\")\n",
    "    \n",
    "    complexity_level: Literal[\"easy\", \"medium\", \"hard\"] = Field(\n",
    "        description=\"Complexity: easy (single-hop), medium (2-3 hops), hard (multi-hop synthesis)\"\n",
    "    )\n",
    "    \n",
    "    reasoning_requirement: Literal[\"simple\", \"contextual\", \"reasoning\", \"synthesis\"] = Field(\n",
    "        description=\"Reasoning type: simple (fact lookup), contextual (understand context), reasoning (logical inference), synthesis (combine multiple pieces)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomQuestion(BaseModel):\n",
    "    \"\"\"Structured output for custom generated questions\"\"\"\n",
    "    question: str = Field(description=\"The generated question\")\n",
    "    answer: str = Field(description=\"Complete answer based on the context\")\n",
    "    question_type: str = Field(description=\"Type of question generated\")\n",
    "    complexity: Literal[\"easy\", \"medium\", \"hard\"] = Field(\n",
    "        default=\"medium\",\n",
    "        description=\"Complexity level of the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FollowupQuestion(BaseModel):\n",
    "    \"\"\"Structured output for follow-up questions\"\"\"\n",
    "    followup_question: str = Field(description=\"The follow-up question\")\n",
    "    followup_answer: str = Field(description=\"Answer to the follow-up question\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================\n",
    "\n",
    "@dataclass\n",
    "class HybridConfig:\n",
    "    \"\"\"Hybrid Generation Configuration\"\"\"\n",
    "    \n",
    "    # Azure OpenAI\n",
    "    azure_endpoint: str = \"https://your-endpoint.openai.azure.com/\"\n",
    "    azure_api_key: str = \"your-api-key\"\n",
    "    azure_api_version: str = \"2024-02-01\"\n",
    "    azure_deployment_gpt4: str = \"gpt-4\"\n",
    "    azure_deployment_embedding: str = \"text-embedding-ada-002\"\n",
    "    \n",
    "    # Coverage\n",
    "    num_spatial_regions: int = 4\n",
    "    questions_per_document: int = 50\n",
    "    questions_per_region: int = 5\n",
    "    \n",
    "    # Chunking\n",
    "    chunk_size: int = 1500\n",
    "    chunk_overlap: int = 300\n",
    "    \n",
    "    # Generation Split\n",
    "    ragas_percentage: float = 0.50  # 70% RAGAS\n",
    "    custom_percentage: float = 0.50  # 30% Custom\n",
    "    \n",
    "    # Custom Question Types\n",
    "    custom_question_types: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"chatbot_style\": 0.40,\n",
    "        \"scenario\": 0.20,\n",
    "        \"multi_hop\": 0.20,\n",
    "        \"comparative\": 0.10,\n",
    "        \"conditional\": 0.10,\n",
    "    })\n",
    "    \n",
    "    # Multi-turn\n",
    "    custom_multiturn_ratio: float = 0.50\n",
    "    max_turns_per_conversation: int = 2\n",
    "    \n",
    "    # Domain\n",
    "    domain_name: str = \"Banking and Financial Services (BIS)\"\n",
    "    domain_context: str = \"BIS Meeting Services, compliance, operations\"\n",
    "    \n",
    "    # Quality\n",
    "    min_quality_score: float = 7.0\n",
    "    \n",
    "    # LLM\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c39e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Credentials configured\n",
      "‚úÖ Configuration loaded\n",
      "   Total questions per doc: 50\n",
      "   RAGAS: 25 (70%)\n",
      "   Custom: 25 (30%)\n",
      "\n",
      "üìç Initializing LLMs...\n",
      "‚úÖ LLMs initialized with structured output\n"
     ]
    }
   ],
   "source": [
    "# Initialize config\n",
    "config = HybridConfig()\n",
    "\n",
    "# Update with your Azure credentials\n",
    "config.azure_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "config.azure_api_key = os.getenv(\"AZURE_OAI_API_KEY\")\n",
    "config.azure_deployment_gpt4 = os.getenv(\"AZURE_OAI_DEPLOYMENT\")\n",
    "config.azure_api_version = os.getenv(\"AZURE_OAI_API_VERSION\")\n",
    "\n",
    "# Update with your Azure credentials\n",
    "config.azure_emb_endpoint = os.getenv(\"AZURE_OAI_EMB_ENDPOINT\")\n",
    "config.azure_emb_api_key = os.getenv(\"AZURE_OAI_EMB_API_KEY\")\n",
    "config.azure_emb_deployment = os.getenv(\"AZURE_OAI_EMB_DEPLOYMENT\")\n",
    "config.azure_emb_version = os.getenv(\"AZURE_OAI_EMB_API_VERSION\")\n",
    "\n",
    "print(\"‚úÖ Credentials configured\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Total questions per doc: {config.questions_per_document}\")\n",
    "print(f\"   RAGAS: {int(config.questions_per_document * config.ragas_percentage)} (70%)\")\n",
    "print(f\"   Custom: {int(config.questions_per_document * config.custom_percentage)} (30%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# INITIALIZE LLMs WITH STRUCTURED OUTPUT\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\nüìç Initializing LLMs...\")\n",
    "\n",
    "# Base LLM\n",
    "base_llm = AzureChatOpenAI(\n",
    "    azure_endpoint=config.azure_endpoint,\n",
    "    api_key=config.azure_api_key,\n",
    "    api_version=config.azure_api_version,\n",
    "    deployment_name=config.azure_deployment_gpt4,\n",
    "    temperature=0.7,\n",
    "    max_tokens=config.max_tokens,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Azure OpenAI Embeddings\n",
    "embeddings_model = AzureOpenAIEmbeddings(\n",
    "    model=os.getenv(\"AZURE_OAI_EMB_DEPLOYMENT\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OAI_EMB_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OAI_EMB_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OAI_EMB_API_VERSION\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Structured LLMs\n",
    "classification_llm = base_llm.with_structured_output(QuestionClassification)\n",
    "custom_question_llm = base_llm.with_structured_output(CustomQuestion)\n",
    "followup_llm = base_llm.with_structured_output(FollowupQuestion)\n",
    "\n",
    "\n",
    "print(\"‚úÖ LLMs initialized with structured output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b961a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8586e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================================\n",
    "\n",
    "def load_documents(documents_dir: str) -> List[Document]:\n",
    "    \"\"\"Load PDF documents\"\"\"\n",
    "    documents = []\n",
    "    doc_dir = Path(documents_dir)\n",
    "    \n",
    "    pdf_files = list(doc_dir.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        return documents\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Loading PDFs\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source_file\"] = pdf_file.name\n",
    "            documents.extend(docs)\n",
    "            print(f\"  ‚úì {pdf_file.name}: {len(docs)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {pdf_file.name}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def chunk_documents(documents: List[Document], config: HybridConfig) -> Dict[str, List[Document]]:\n",
    "    \"\"\"Chunk documents by source file\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.chunk_size,\n",
    "        chunk_overlap=config.chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    \n",
    "    docs_by_file = {}\n",
    "    for doc in documents:\n",
    "        source = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        if source not in docs_by_file:\n",
    "            docs_by_file[source] = []\n",
    "        docs_by_file[source].append(doc)\n",
    "    \n",
    "    chunked_docs = {}\n",
    "    for source, docs in docs_by_file.items():\n",
    "        combined_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        combined_doc = Document(page_content=combined_text, metadata={\"source_file\": source})\n",
    "        chunks = text_splitter.split_documents([combined_doc])\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"chunk_index\"] = i\n",
    "            chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "        \n",
    "        chunked_docs[source] = chunks\n",
    "        print(f\"  ‚úì {source}: {len(chunks)} chunks\")\n",
    "    \n",
    "    return chunked_docs\n",
    "\n",
    "\n",
    "def sample_chunks_spatial(chunks: List[Document], n_samples: int, num_regions: int) -> List[Document]:\n",
    "    \"\"\"Sample chunks evenly from spatial regions\"\"\"\n",
    "    total = len(chunks)\n",
    "    if total == 0:\n",
    "        return []\n",
    "    \n",
    "    region_size = total // num_regions\n",
    "    samples_per_region = n_samples // num_regions\n",
    "    \n",
    "    sampled = []\n",
    "    for region_idx in range(num_regions):\n",
    "        start = region_idx * region_size\n",
    "        end = start + region_size if region_idx < num_regions - 1 else total\n",
    "        region_chunks = chunks[start:end]\n",
    "        \n",
    "        if len(region_chunks) > 0:\n",
    "            step = max(1, len(region_chunks) // samples_per_region)\n",
    "            selected = region_chunks[::step][:samples_per_region]\n",
    "            for chunk in selected:\n",
    "                chunk.metadata[\"region_id\"] = region_idx\n",
    "            sampled.extend(selected)\n",
    "    \n",
    "    return sampled[:n_samples]\n",
    "\n",
    "\n",
    "def sample_chunks_for_both(chunked_docs: Dict, config: HybridConfig):\n",
    "    \"\"\"Sample chunks for both RAGAS and Custom\"\"\"\n",
    "    sampled_for_ragas = {}\n",
    "    sampled_for_custom = {}\n",
    "    \n",
    "    for source, chunks in chunked_docs.items():\n",
    "        ragas_count = int(config.questions_per_document * config.ragas_percentage)\n",
    "        custom_count = int(config.questions_per_document * config.custom_percentage)\n",
    "        \n",
    "        # Sample for RAGAS\n",
    "        sampled_for_ragas[source] = sample_chunks_spatial(chunks, ragas_count, config.num_spatial_regions)\n",
    "        \n",
    "        # Sample for Custom (different chunks)\n",
    "        all_indices = set(range(len(chunks)))\n",
    "        ragas_indices = set([c.metadata['chunk_index'] for c in sampled_for_ragas[source]])\n",
    "        remaining_indices = list(all_indices - ragas_indices)\n",
    "        \n",
    "        if len(remaining_indices) >= custom_count:\n",
    "            step = max(1, len(remaining_indices) // custom_count)\n",
    "            selected_indices = remaining_indices[::step][:custom_count]\n",
    "            sampled_for_custom[source] = [chunks[i] for i in selected_indices]\n",
    "        else:\n",
    "            sampled_for_custom[source] = sample_chunks_spatial(chunks, custom_count, config.num_spatial_regions)\n",
    "        \n",
    "        print(f\"{source}: RAGAS={len(sampled_for_ragas[source])}, Custom={len(sampled_for_custom[source])}\")\n",
    "    \n",
    "    return sampled_for_ragas, sampled_for_custom\n",
    "\n",
    "\n",
    "def generate_ragas_questions(llm, embeddings, sampled_chunks, test_mode, test_count):\n",
    "    \"\"\"Generate questions using RAGAS v0.1.21\"\"\"\n",
    "    all_questions = []\n",
    "    \n",
    "    # Initialize generator (v0.1.21 API)\n",
    "    generator = TestsetGenerator.from_langchain(\n",
    "        generator_llm=llm,\n",
    "        critic_llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "    for source_file, chunks in sampled_chunks.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"RAGAS: {source_file}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if test_mode:\n",
    "            chunks = chunks[:test_count]\n",
    "        \n",
    "        testset_size = len(chunks)\n",
    "        print(f\"Generating {testset_size} questions...\")\n",
    "        \n",
    "        try:\n",
    "            # v0.1.21 API: generate_with_langchain_docs\n",
    "            # distributions parameter controls question types:\n",
    "            #   - simple: Simple factual questions (direct retrieval)\n",
    "            #   - reasoning: Questions requiring reasoning/inference\n",
    "            #   - multi_context: Questions requiring multiple document chunks\n",
    "            # Adjust ratios as needed (must sum to 1.0):\n",
    "            #   Example: {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "            testset = generator.generate_with_langchain_docs(\n",
    "                chunks, \n",
    "                test_size=testset_size,\n",
    "                distributions={simple: 0.25, reasoning: 0.25, multi_context: 0.50}\n",
    "            )\n",
    "            \n",
    "            # Convert to pandas\n",
    "            df = testset.to_pandas()\n",
    "            df[\"source_file\"] = source_file\n",
    "            df[\"generation_method\"] = \"ragas\"\n",
    "            all_questions.append(df)\n",
    "            print(f\"‚úÖ Generated {len(df)} questions\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    if all_questions:\n",
    "        return pd.concat(all_questions, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def generate_custom_questions_structured(sampled_chunks, question_llm, followup_llm, config, test_mode, test_count):\n",
    "    \"\"\"Generate custom questions using structured output\"\"\"\n",
    "    all_questions = []\n",
    "    \n",
    "    for source_file, chunks in sampled_chunks.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Custom: {source_file}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if test_mode:\n",
    "            chunks = chunks[:test_count]\n",
    "        \n",
    "        # Get question type sequence\n",
    "        q_types = get_type_sequence(len(chunks), config.custom_question_types)\n",
    "        \n",
    "        # Determine multi-turn\n",
    "        n_multiturn = int(len(q_types) * config.custom_multiturn_ratio)\n",
    "        multiturn_indices = np.random.choice(len(q_types), n_multiturn, replace=False)\n",
    "        \n",
    "        doc_questions = []\n",
    "        \n",
    "        for idx, (chunk, q_type) in enumerate(tqdm(list(zip(chunks, q_types)), desc=\"Custom Gen\")):\n",
    "            # Generate question with structured output\n",
    "            qa = generate_custom_question_with_structure(chunk, q_type, question_llm, config)\n",
    "            \n",
    "            if qa:\n",
    "                qa['conversation_type'] = 'multi_turn' if idx in multiturn_indices else 'single_turn'\n",
    "                qa['generation_method'] = 'custom'\n",
    "                qa['has_followup'] = False\n",
    "                qa['followup_questions'] = []\n",
    "                \n",
    "                # Generate follow-up if multi-turn\n",
    "                if idx in multiturn_indices:\n",
    "                    followup = generate_followup_with_structure(qa, chunk, followup_llm)\n",
    "                    if followup:\n",
    "                        qa['has_followup'] = True\n",
    "                        qa['followup_questions'] = [followup]\n",
    "                \n",
    "                doc_questions.append(qa)\n",
    "        \n",
    "        all_questions.extend(doc_questions)\n",
    "        print(f\"‚úÖ Generated {len(doc_questions)} questions\")\n",
    "        print(f\"   Multi-turn: {sum(1 for q in doc_questions if q['has_followup'])}\")\n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "\n",
    "def generate_custom_question_with_structure(chunk: Document, q_type: str, llm, config) -> Optional[Dict]:\n",
    "    \"\"\"Generate custom question using structured output\"\"\"\n",
    "    prompt = f\"\"\"You are generating test questions for {config.domain_name}.\n",
    "\n",
    "CONTEXT:\n",
    "{chunk.page_content[:2500]}\n",
    "\n",
    "QUESTION TYPE: {q_type}\n",
    "\n",
    "TYPE DEFINITIONS:\n",
    "- chatbot_style: Conversational (\"Can you help me understand...?\")\n",
    "- scenario: Realistic situation (\"A client requests... What should...?\")\n",
    "- multi_hop: Requires multiple pieces (\"If X and Y, then what...?\")\n",
    "- comparative: Compare items (\"What's the difference between...?\")\n",
    "- conditional: If-then (\"If this happens, what are the consequences?\")\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Be SPECIFIC and PROFESSIONAL\n",
    "- Include domain context\n",
    "- Make it ANSWERABLE from the context\n",
    "\n",
    "Generate ONE {q_type} question with its complete answer.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result: CustomQuestion = llm.invoke(prompt)\n",
    "        return {\n",
    "            'question': result.question,\n",
    "            'answer': result.answer,\n",
    "            'question_type': result.question_type,\n",
    "            'complexity': result.complexity,\n",
    "            'source_file': chunk.metadata.get('source_file'),\n",
    "            'region_id': chunk.metadata.get('region_id', 0)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error: {str(e)[:80]}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_followup_with_structure(initial_qa: Dict, chunk: Document, llm) -> Optional[Dict]:\n",
    "    \"\"\"Generate follow-up question using structured output\"\"\"\n",
    "    prompt = f\"\"\"Generate a FOLLOW-UP question.\n",
    "\n",
    "CONTEXT: {chunk.page_content[:2500]}\n",
    "INITIAL Q: {initial_qa['question']}\n",
    "INITIAL A: {initial_qa['answer']}\n",
    "\n",
    "Generate a natural follow-up that builds on the initial question.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result: FollowupQuestion = llm.invoke(prompt)\n",
    "        return {\n",
    "            'followup_question': result.followup_question,\n",
    "            'followup_answer': result.followup_answer\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def classify_ragas_questions_structured(ragas_df: pd.DataFrame, llm) -> pd.DataFrame:\n",
    "    \"\"\"Classify RAGAS questions using structured output\"\"\"\n",
    "    print(f\"\\nClassifying {len(ragas_df)} RAGAS questions...\")\n",
    "    \n",
    "    classifications = []\n",
    "    for idx, row in tqdm(ragas_df.iterrows(), total=len(ragas_df), desc=\"Classifying\"):\n",
    "        classification = classify_question_with_structure(\n",
    "            row['question'],\n",
    "            row.get('answer', row.get('ground_truth', '')),\n",
    "            llm\n",
    "        )\n",
    "        classifications.append(classification)\n",
    "    \n",
    "    class_df = pd.DataFrame(classifications)\n",
    "    ragas_df = pd.concat([ragas_df, class_df], axis=1)\n",
    "    \n",
    "    # Add metadata\n",
    "    ragas_df['conversation_type'] = 'single_turn'\n",
    "    ragas_df['is_followup'] = False\n",
    "    ragas_df['turn_number'] = 1\n",
    "    ragas_df['parent_question_id'] = None\n",
    "    \n",
    "    return ragas_df\n",
    "\n",
    "\n",
    "def classify_question_with_structure(question: str, answer: str, llm) -> Dict:\n",
    "    \"\"\"Classify a question using structured output\"\"\"\n",
    "    prompt = f\"\"\"You are a question classifier for RAG evaluation test sets.\n",
    "\n",
    "QUESTION: {question}\n",
    "ANSWER: {answer[:500]}\n",
    "\n",
    "Analyze this question and classify it according to the following categories:\n",
    "\n",
    "QUESTION STYLE (choose ONE):\n",
    "- chatbot_style: Conversational, help-seeking\n",
    "- direct_factual: Direct fact questions\n",
    "- procedural: Process/how-to questions\n",
    "- scenario: Realistic situation-based\n",
    "- analytical: Analysis/evaluation\n",
    "- compliance: Regulatory/policy\n",
    "- descriptive: Detailed description\n",
    "- multi_hop: Requires multiple pieces of info\n",
    "- comparative: Comparison questions\n",
    "- conditional: If-then scenarios\n",
    "\n",
    "COMPLEXITY LEVEL:\n",
    "- easy: Single-hop, direct retrieval\n",
    "- medium: 2-3 hops, some reasoning\n",
    "- hard: Multi-hop, synthesis required\n",
    "\n",
    "REASONING REQUIREMENT:\n",
    "- simple: Direct fact lookup\n",
    "- contextual: Requires understanding context\n",
    "- reasoning: Requires logical reasoning\n",
    "- synthesis: Requires combining multiple pieces\n",
    "\n",
    "Classify this question.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result: QuestionClassification = llm.invoke(prompt)\n",
    "        return {\n",
    "            'question_style': result.question_style,\n",
    "            'question_type': result.question_type,\n",
    "            'complexity_level': result.complexity_level,\n",
    "            'reasoning_requirement': result.reasoning_requirement\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error: {str(e)[:60]}\")\n",
    "        return {\n",
    "            'question_style': 'direct_factual',\n",
    "            'question_type': 'direct_factual',\n",
    "            'complexity_level': 'medium',\n",
    "            'reasoning_requirement': 'contextual'\n",
    "        }\n",
    "\n",
    "\n",
    "def reclassify_custom_questions_structured(custom_questions: List[Dict], llm) -> List[Dict]:\n",
    "    \"\"\"Re-classify custom questions using structured output\"\"\"\n",
    "    print(f\"\\nRe-classifying {len(custom_questions)} custom questions...\")\n",
    "    \n",
    "    for qa in tqdm(custom_questions, desc=\"Re-classifying\"):\n",
    "        classification = classify_question_with_structure(qa['question'], qa['answer'], llm)\n",
    "        qa.update(classification)\n",
    "    \n",
    "    return custom_questions\n",
    "\n",
    "\n",
    "def get_type_sequence(n: int, distribution: Dict[str, float]) -> List[str]:\n",
    "    \"\"\"Create question type sequence\"\"\"\n",
    "    seq = []\n",
    "    for q_type, ratio in distribution.items():\n",
    "        count = max(1, int(n * ratio))\n",
    "        seq.extend([q_type] * count)\n",
    "    import random\n",
    "    random.shuffle(seq)\n",
    "    return seq[:n]\n",
    "\n",
    "\n",
    "def merge_datasets(ragas_df: pd.DataFrame, custom_questions: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Merge RAGAS and Custom datasets\"\"\"\n",
    "    \n",
    "    # Flatten custom questions\n",
    "    custom_flat = []\n",
    "    for qa in custom_questions:\n",
    "        initial = {\n",
    "            'question': qa['question'],\n",
    "            'answer': qa['answer'],\n",
    "            'source_file': qa.get('source_file'),\n",
    "            'region_id': qa.get('region_id', 0) + 1,\n",
    "            'question_type': qa.get('question_type'),\n",
    "            'question_style': qa.get('question_style', qa.get('question_type')),\n",
    "            'complexity_level': qa.get('complexity_level', qa.get('complexity', 'medium')),\n",
    "            'reasoning_requirement': qa.get('reasoning_requirement', 'contextual'),\n",
    "            'conversation_type': qa.get('conversation_type'),\n",
    "            'generation_method': 'custom',\n",
    "            'is_followup': False,\n",
    "            'turn_number': 1,\n",
    "            'parent_question_id': None,\n",
    "        }\n",
    "        custom_flat.append(initial)\n",
    "        \n",
    "        # Follow-ups\n",
    "        if qa.get('has_followup'):\n",
    "            parent_idx = len(custom_flat) - 1\n",
    "            for followup in qa.get('followup_questions', []):\n",
    "                fu = {\n",
    "                    'question': followup['followup_question'],\n",
    "                    'answer': followup['followup_answer'],\n",
    "                    'source_file': qa.get('source_file'),\n",
    "                    'region_id': qa.get('region_id', 0) + 1,\n",
    "                    'question_type': 'followup',\n",
    "                    'question_style': 'followup',\n",
    "                    'complexity_level': qa.get('complexity_level', 'medium'),\n",
    "                    'reasoning_requirement': qa.get('reasoning_requirement', 'contextual'),\n",
    "                    'conversation_type': 'multi_turn',\n",
    "                    'generation_method': 'custom',\n",
    "                    'is_followup': True,\n",
    "                    'turn_number': 2,\n",
    "                    'parent_question_id': parent_idx,\n",
    "                }\n",
    "                custom_flat.append(fu)\n",
    "    \n",
    "    custom_df = pd.DataFrame(custom_flat)\n",
    "    \n",
    "    # Standardize RAGAS\n",
    "    if len(ragas_df) > 0:\n",
    "        ragas_standardized = ragas_df.rename(columns={'ground_truth': 'answer'})\n",
    "        if 'region_id' not in ragas_standardized.columns:\n",
    "            ragas_standardized['region_id'] = 0\n",
    "    else:\n",
    "        ragas_standardized = pd.DataFrame()\n",
    "    \n",
    "    # Combine\n",
    "    all_dfs = []\n",
    "    if len(ragas_standardized) > 0:\n",
    "        all_dfs.append(ragas_standardized)\n",
    "    if len(custom_df) > 0:\n",
    "        all_dfs.append(custom_df)\n",
    "    \n",
    "    if all_dfs:\n",
    "        common_cols = ['question', 'answer', 'source_file', 'generation_method',\n",
    "                       'question_type', 'question_style', 'complexity_level', 'reasoning_requirement',\n",
    "                       'conversation_type', 'is_followup', 'turn_number', 'parent_question_id']\n",
    "        \n",
    "        for df in all_dfs:\n",
    "            for col in common_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = 'contextual' if col == 'reasoning_requirement' else None\n",
    "        \n",
    "        final_df = pd.concat([df[common_cols] for df in all_dfs], ignore_index=True)\n",
    "        final_df.insert(0, 'question_id', [f\"Q{i+1}\" for i in range(len(final_df))])\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def print_coverage_analysis(df: pd.DataFrame):\n",
    "    \"\"\"Print coverage analysis\"\"\"\n",
    "    print(\"\\nüìä COVERAGE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTotal questions: {len(df)}\")\n",
    "    print(f\"RAGAS: {len(df[df['generation_method'] == 'ragas'])}\")\n",
    "    print(f\"Custom: {len(df[df['generation_method'] == 'custom'])}\")\n",
    "    print(f\"\\nQuestion types: {df['question_type'].nunique()} unique\")\n",
    "    print(df['question_type'].value_counts().head(10))\n",
    "    print(f\"\\nComplexity: {df['complexity_level'].value_counts().to_dict()}\")\n",
    "    print(f\"Reasoning: {df['reasoning_requirement'].value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "def print_sample_questions(df: pd.DataFrame):\n",
    "    \"\"\"Print sample questions\"\"\"\n",
    "    print(\"\\nüìã SAMPLE QUESTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ragas_samples = df[df['generation_method'] == 'ragas'].head(2)\n",
    "    custom_samples = df[df['generation_method'] == 'custom'].head(2)\n",
    "    \n",
    "    print(\"\\nüî∑ RAGAS QUESTIONS:\\n\")\n",
    "    for _, row in ragas_samples.iterrows():\n",
    "        print(f\"Q: {row['question']}\")\n",
    "        print(f\"Type: {row['question_type']}\")\n",
    "        print(f\"A: {row['answer'][:150]}...\\n\")\n",
    "    \n",
    "    print(\"\\nüî∂ CUSTOM QUESTIONS:\\n\")\n",
    "    for _, row in custom_samples.iterrows():\n",
    "        print(f\"Q: {row['question']}\")\n",
    "        print(f\"Type: {row['question_type']}\")\n",
    "        print(f\"Conv: {row['conversation_type']}\")\n",
    "        print(f\"A: {row['answer'][:150]}...\\n\")\n",
    "\n",
    "\n",
    "def export_datasets(df: pd.DataFrame):\n",
    "    \"\"\"Export datasets to files\"\"\"\n",
    "    OUTPUT_DIR = \"./outputs\"\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Full dataset\n",
    "    full_file = f\"{OUTPUT_DIR}/testset_hybrid_full.csv\"\n",
    "    df.to_csv(full_file, index=False)\n",
    "    print(f\"\\n‚úÖ Exported: {full_file}\")\n",
    "    \n",
    "    # Simple format\n",
    "    simple_df = df[['question', 'answer', 'source_file']].copy()\n",
    "    simple_df.columns = ['question', 'ground_truth', 'source']\n",
    "    simple_file = f\"{OUTPUT_DIR}/testset_hybrid_simple.csv\"\n",
    "    simple_df.to_csv(simple_file, index=False)\n",
    "    print(f\"‚úÖ Exported: {simple_file}\")\n",
    "    \n",
    "    # Summary\n",
    "    summary = {\n",
    "        'total_questions': len(df),\n",
    "        'ragas_count': len(df[df['generation_method'] == 'ragas']),\n",
    "        'custom_count': len(df[df['generation_method'] == 'custom']),\n",
    "        'question_types': df['question_type'].value_counts().to_dict(),\n",
    "        'complexity': df['complexity_level'].value_counts().to_dict(),\n",
    "        'reasoning': df['reasoning_requirement'].value_counts().to_dict(),\n",
    "    }\n",
    "    \n",
    "    summary_file = f\"{OUTPUT_DIR}/testset_hybrid_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"‚úÖ Exported: {summary_file}\")\n",
    "\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame):\n",
    "    \"\"\"Create visualizations\"\"\"\n",
    "    OUTPUT_DIR = \"./outputs\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # RAGAS vs Custom\n",
    "    method_counts = df['generation_method'].value_counts()\n",
    "    axes[0, 0].pie(method_counts.values, labels=method_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('RAGAS vs Custom Distribution')\n",
    "    \n",
    "    # Question types\n",
    "    type_counts = df['question_type'].value_counts().head(8)\n",
    "    axes[0, 1].barh(range(len(type_counts)), type_counts.values)\n",
    "    axes[0, 1].set_yticks(range(len(type_counts)))\n",
    "    axes[0, 1].set_yticklabels(type_counts.index)\n",
    "    axes[0, 1].set_title('Question Types')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    \n",
    "    # Complexity\n",
    "    complexity_counts = df['complexity_level'].value_counts()\n",
    "    axes[1, 0].bar(range(len(complexity_counts)), complexity_counts.values)\n",
    "    axes[1, 0].set_xticks(range(len(complexity_counts)))\n",
    "    axes[1, 0].set_xticklabels(complexity_counts.index)\n",
    "    axes[1, 0].set_title('Complexity Distribution')\n",
    "    \n",
    "    # Reasoning\n",
    "    reasoning_counts = df['reasoning_requirement'].value_counts()\n",
    "    axes[1, 1].bar(range(len(reasoning_counts)), reasoning_counts.values)\n",
    "    axes[1, 1].set_xticks(range(len(reasoning_counts)))\n",
    "    axes[1, 1].set_xticklabels(reasoning_counts.index)\n",
    "    axes[1, 1].set_title('Reasoning Requirements')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz_file = f\"{OUTPUT_DIR}/testset_hybrid_analysis.png\"\n",
    "    plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Visualization saved: {viz_file}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f590e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45330b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Test: LLM generator working\n",
      "‚úÖ Embeddings Test: Generated 3072 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Test LLM\n",
    "test_response = base_llm.invoke(\"Say 'LLM generator working'\")\n",
    "print(f\"‚úÖ LLM Test: {test_response.content}\")\n",
    "\n",
    "\n",
    "\n",
    "# Test Embeddings\n",
    "test_embedding = embeddings_model.embed_query(\"test\")\n",
    "print(f\"‚úÖ Embeddings Test: Generated {len(test_embedding)} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81643cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì othp33.pdf: 26 pages\n",
      "‚úÖ Loaded 26 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìç Loading documents...\")\n",
    "    \n",
    "DOCUMENTS_DIR = \"./documents\"\n",
    "Path(DOCUMENTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "documents = load_documents(DOCUMENTS_DIR)\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7804b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Chunking documents...\n",
      "  ‚úì othp33.pdf: 69 chunks\n",
      "‚úÖ Chunking complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìç Chunking documents...\")\n",
    "chunked_docs_by_file = chunk_documents(documents, config)\n",
    "print(\"‚úÖ Chunking complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d1d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Sampling chunks...\n",
      "othp33.pdf: RAGAS=24, Custom=25\n",
      "‚úÖ Sampling complete\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# SAMPLE CHUNKS\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\nüìç Sampling chunks...\")\n",
    "sampled_for_ragas, sampled_for_custom = sample_chunks_for_both(\n",
    "    chunked_docs_by_file, config\n",
    ")\n",
    "print(\"‚úÖ Sampling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285f8927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç RAGAS Generation (70%)...\n",
      "‚ö†Ô∏è  TEST MODE: Generating 7 RAGAS questions per doc\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generator_llm_wrapped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TEST_MODE:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è  TEST MODE: Generating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_RAGAS_PER_DOC\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m RAGAS questions per doc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m ragas_generator = TestsetGenerator(\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     llm=\u001b[43mgenerator_llm_wrapped\u001b[49m,\n\u001b[32m     15\u001b[39m     embedding_model=embeddings_wrapped,\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m ragas_df = generate_ragas_questions(\n\u001b[32m     19\u001b[39m     base_llm, embeddings_model, sampled_for_ragas, TEST_MODE, TEST_RAGAS_PER_DOC\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ RAGAS generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ragas_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m questions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'generator_llm_wrapped' is not defined"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# RAGAS GENERATION (70%)\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\nüìç RAGAS Generation (70%)...\")\n",
    "\n",
    "TEST_MODE = True  # Set to False for full generation\n",
    "TEST_RAGAS_PER_DOC = 7\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"‚ö†Ô∏è  TEST MODE: Generating {TEST_RAGAS_PER_DOC} RAGAS questions per doc\")\n",
    "\n",
    "ragas_generator = TestsetGenerator(\n",
    "    llm=generator_llm_wrapped,\n",
    "    embedding_model=embeddings_wrapped,\n",
    ")\n",
    "\n",
    "ragas_df = generate_ragas_questions(\n",
    "    base_llm, embeddings_model, sampled_for_ragas, TEST_MODE, TEST_RAGAS_PER_DOC\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ RAGAS generated {len(ragas_df)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55b15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    print(\"\\nüìç Chunking documents...\")\n",
    "    chunked_docs_by_file = chunk_documents(documents, config)\n",
    "    print(\"‚úÖ Chunking complete\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # SAMPLE CHUNKS\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\nüìç Sampling chunks...\")\n",
    "    sampled_for_ragas, sampled_for_custom = sample_chunks_for_both(\n",
    "        chunked_docs_by_file, config\n",
    "    )\n",
    "    print(\"‚úÖ Sampling complete\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # RAGAS GENERATION (70%)\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\nüìç RAGAS Generation (70%)...\")\n",
    "    \n",
    "    TEST_MODE = True  # Set to False for full generation\n",
    "    TEST_RAGAS_PER_DOC = 7\n",
    "    \n",
    "    if TEST_MODE:\n",
    "        print(f\"‚ö†Ô∏è  TEST MODE: Generating {TEST_RAGAS_PER_DOC} RAGAS questions per doc\")\n",
    "    \n",
    "    ragas_generator = TestsetGenerator(\n",
    "        llm=generator_llm_wrapped,\n",
    "        embedding_model=embeddings_wrapped,\n",
    "    )\n",
    "    \n",
    "    ragas_df = generate_ragas_questions(\n",
    "        base_llm, embeddings_model, sampled_for_ragas, TEST_MODE, TEST_RAGAS_PER_DOC\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ RAGAS generated {len(ragas_df)} questions\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # CUSTOM GENERATION (30%) WITH STRUCTURED OUTPUT\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\nüìç Custom Generation (30%) with structured output...\")\n",
    "    \n",
    "    TEST_CUSTOM_PER_DOC = 3\n",
    "    \n",
    "    custom_questions_all = generate_custom_questions_structured(\n",
    "        sampled_for_custom,\n",
    "        custom_question_llm,\n",
    "        followup_llm,\n",
    "        config,\n",
    "        TEST_MODE,\n",
    "        TEST_CUSTOM_PER_DOC\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Custom generated {len(custom_questions_all)} questions\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # CLASSIFY ALL QUESTIONS WITH STRUCTURED OUTPUT\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\nüìç Classifying questions with structured output...\")\n",
    "    \n",
    "    # Classify RAGAS questions\n",
    "    if len(ragas_df) > 0:\n",
    "        ragas_df = classify_ragas_questions_structured(\n",
    "            ragas_df, classification_llm\n",
    "        )\n",
    "        print(f\"‚úÖ Classified {len(ragas_df)} RAGAS questions\")\n",
    "    \n",
    "    # Re-classify Custom questions (optional)\n",
    "    RECLASSIFY_CUSTOM = True\n",
    "    \n",
    "    if RECLASSIFY_CUSTOM and len(custom_questions_all) > 0:\n",
    "        custom_questions_all = reclassify_custom_questions_structured(\n",
    "            custom_questions_all, classification_llm\n",
    "        )\n",
    "        print(f\"‚úÖ Re-classified {len(custom_questions_all)} custom questions\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # MERGE & CREATE FINAL DATASET\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\nüìç Merging datasets...\")\n",
    "    \n",
    "    final_df = merge_datasets(ragas_df, custom_questions_all)\n",
    "    \n",
    "    if len(final_df) > 0:\n",
    "        print(f\"‚úÖ Final dataset: {len(final_df)} questions\")\n",
    "        print(f\"   RAGAS: {len(final_df[final_df['generation_method'] == 'ragas'])}\")\n",
    "        print(f\"   Custom: {len(final_df[final_df['generation_method'] == 'custom'])}\")\n",
    "        \n",
    "        # Coverage analysis\n",
    "        print_coverage_analysis(final_df)\n",
    "        \n",
    "        # Sample questions\n",
    "        print_sample_questions(final_df)\n",
    "        \n",
    "        # Export\n",
    "        export_datasets(final_df)\n",
    "        \n",
    "        # Visualize\n",
    "        create_visualizations(final_df)\n",
    "        \n",
    "        print(\"\\n‚úÖ Complete! Check ./outputs/ for generated files\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No questions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e61d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
